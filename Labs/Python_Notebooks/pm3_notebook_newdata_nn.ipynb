{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010003,
     "end_time": "2021-03-28T13:27:45.376868",
     "exception": false,
     "start_time": "2021-03-28T13:27:45.366865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "This notebook contains an example for teaching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a",
    "papermill": {
     "duration": 0.008782,
     "end_time": "2021-03-28T13:27:45.395504",
     "exception": false,
     "start_time": "2021-03-28T13:27:45.386722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A Simple Case Study using Wage Data from 2015 - proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008735,
     "end_time": "2021-03-28T13:27:45.413302",
     "exception": false,
     "start_time": "2021-03-28T13:27:45.404567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So far we considered many machine learning method, e.g Lasso and Random Forests, to build a predictive model. In this lab, we extend our toolbox by predicting wages by a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009256,
     "end_time": "2021-03-28T13:27:45.431663",
     "exception": false,
     "start_time": "2021-03-28T13:27:45.422407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0088,
     "end_time": "2021-03-28T13:27:45.449606",
     "exception": false,
     "start_time": "2021-03-28T13:27:45.440806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Again, we consider data from the U.S. March Supplement of the Current Population Survey (CPS) in 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This notebook contains an example for teaching.\n",
    "\n",
    "\n",
    "# A Simple Case Study using Wage Data from 2015 - proceeding\n",
    "\n",
    "So far we considered many machine learning method, e.g Lasso and Random Forests, to build a predictive model. In this lab, we extend our toolbox by predicting wages by a neural network.\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "Again, we consider data from the U.S. March Supplement of the Current Population Survey (CPS) in 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyreadr.librdata'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27044/3775214174.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyreadr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpatsy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_2\\lib\\site-packages\\pyreadr\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpyreadr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_rds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_rdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcustom_errors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPyreadrError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLibrdataError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"0.4.0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_2\\lib\\site-packages\\pyreadr\\pyreadr.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_pyreadr_parser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPyreadrParser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mListObjectsParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_pyreadr_writer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPyreadrWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcustom_errors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPyreadrError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow_2\\lib\\site-packages\\pyreadr\\_pyreadr_parser.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlibrdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcustom_errors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPyreadrError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyreadr.librdata'"
     ]
    }
   ],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "from sklearn import preprocessing\n",
    "import patsy\n",
    "\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata_read = pyreadr.read_r(\"../data/wage2015_subsample_inference.Rdata\")\n",
    "data = rdata_read[ 'data' ]\n",
    "n = data.shape[0]\n",
    "\n",
    "type(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages for splitting data\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# Set Seed\n",
    "# to make the results replicable (generating random numbers)\n",
    "np.random.seed(0)\n",
    "random = np.random.randint(0, data.shape[0], size=math.floor(data.shape[0]))\n",
    "data[\"random\"] = random\n",
    "random    # the array does not change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = data.sort_values(by=['random'])\n",
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing sample \n",
    "data_train = data_2[ : math.floor(n*3/4)]    # training sample\n",
    "data_test =  data_2[ math.floor(n*3/4) : ]   # testing sample\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.iloc[:, 0:16]\n",
    "data_test = data_test.iloc[:, 0:16] \n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler =  MinMaxScaler().fit(data_train)\n",
    "scaler =  MinMaxScaler().fit(data_test)\n",
    "\n",
    "# scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "# scaler = preprocessing.StandardScaler().fit(data_test)\n",
    "\n",
    "data_train_scaled = scaler.transform(data_train)\n",
    "data_test_scaled = scaler.transform(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled = pd.DataFrame(data_train_scaled, columns = columns)\n",
    "data_test_scaled = pd.DataFrame(data_test_scaled, columns = columns)\n",
    "data_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we construct the inputs for our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_basic = \"lwage ~ sex + exp1 + shs + hsg+ scl + clg + mw + so + we\"\n",
    "Y_train, model_X_basic_train = patsy.dmatrices(formula_basic, data_train_scaled, return_type='dataframe')\n",
    "Y_test, model_X_basic_test = patsy.dmatrices(formula_basic, data_test_scaled, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to determine the structure of our network. We are using the R/python package *keras* to build a simple sequential neural network with three dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_X_basic_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim = model_X_basic_train.shape[1], activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "# model.add(Dense(5, activation = 'relu'))\n",
    "\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "opt = keras.optimizers.Adam(learning_rate=0.005)\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mae = tf.keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\", dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the structure of our network in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=mse, optimizer= opt , metrics=mae)\n",
    "model.summary(line_length=None, positions=None, print_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth to notice that we have in total $441$ trainable parameters.\n",
    "\n",
    "Now, let us train the network. Note that this takes some computation time. Thus, we are using gpu to speed up. The exact speed-up varies based on a number of factors including model architecture, batch-size, input pipeline complexity, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the keras model on the dataset\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this [link](https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network), to understand batch_size argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(model_X_basic_train, Y_train, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(model_X_basic_test, Y_test, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nn = model.predict(model_X_basic_test)\n",
    "pred_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_basic = (Y_test-pred_nn)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_nn_basic = sm.OLS( resid_basic , np.ones( resid_basic.shape[0] ) ).fit().summary2().tables[1].iloc[0, 0:2]\n",
    "MSE_nn_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_nn_basic = 1 - ( MSE_nn_basic[0]/Y_test.var() )\n",
    "print( f\"The R^2 using NN is equal to = {R2_nn_basic[0]}\" ) # MSE NN (basic model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
