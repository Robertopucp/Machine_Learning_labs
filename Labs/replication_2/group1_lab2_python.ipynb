{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Members:\n",
    "* Stephy Riega - 20171426\n",
    "* Jesus Soto - 20172738\n",
    "* Franco Caceres - 2016615"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019635,
     "end_time": "2021-02-18T19:28:30.110372",
     "exception": false,
     "start_time": "2021-02-18T19:28:30.090737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. What is data splitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splitting consists in dividing the data *randomly* in at least two samples, training our models in one and testing their predict capacity in the other one. The first step to evaluate the prediction's performance of the model is data splitting. The proportion represented by each partitioned data varies depending of the researcher, but it is pretty common that the data has a proportion of 80%/20%. You are going to name \"train\" at the first split (which represents  the 80%) and \"test\" (20%) to the other one. It is important to take into account that this data splitting has to be done *randomly*. The second step is to perform the regression or regressions of different models in the training sample and, after that, perform the predict of your dependent variable ($Y_{test-estimated}$) on your test sample using your estimators ($\\beta_{train}$) from the training sample. The third step is to estimate the $R^2$ and $MSE_{OFS}$ of the regressions by calculating the average of the squared difference between the observed ($Y_{test}$) and the estimaded ($Y_{test-estimated}$). With this, you can compared them with $R^2$  and $MSE$ into sample to know if your estimators perform better than in the sample data or which model perform better out of sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Replicate the PM1_Notebook1_Prediction_newdata (R and Python) JN but with restricted data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, Y  is the hourly wage of a worker and  X  is a vector of worker's characteristics, e.g., education, experience, gender. We focus on how to use job-relevant characteristics, such as education and experience, to best predict wages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set we consider is from the March Supplement of the U.S. Current Population Survey, year 2015. We select white non-hispanic individuals, aged 25 to 64 years, and working more than 35 hours per week during at least 50 weeks of the year. We exclude self-employed workers; individuals living in group quarters; individuals in the military, agricultural or private household sectors; individuals with inconsistent reports on earnings and employment status; individuals with allocated or missing information in any of the variables used in the analysis; and individuals with hourly wage below  3 .\n",
    "\n",
    "The variable of interest  Y  is the hourly wage rate constructed as the ratio of the annual earnings to the total number of hours worked, which is constructed in turn as the product of number of weeks worked and the usual number of hours worked per week. In our analysis, we also focus on single (never married) workers. The final sample is of size  **n=5150** ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data \n",
    "rdata_read = pyreadr.read_r(\"../data/wage2015_subsample_inference.Rdata\")\n",
    "data_raw = rdata_read[ 'data' ]\n",
    "type(data_raw)\n",
    "data_raw.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a data frame with 5150 observations with 20 variables. However,  **since we want only a subsample of the data that consists of the people who did not go to college**, we restrict the data set using the variables *shs* and *hsg* (which represents the people who ultimately went partially to high school or completed this level of education). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw.loc[(data_raw[\"shs\"] == 1) | (data_raw[\"hsg\"] == 1)]\n",
    "data.shape #now we have 1376 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info() # info of the new subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we can see that there are no observations of people who went for higher levels of educations than high school (like college or advanced levels- sinced it is necessary college to go a MA or PhD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are constructing the output variable  **Y**  and the matrix  **Z**  which includes the characteristics of workers that are given in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.log2(data['wage']) \n",
    "n = len(Y)\n",
    "z = data.loc[:, ~data.columns.isin(['wage', 'lwage','Unnamed: 0'])]\n",
    "z # the covariates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = z.shape[1] #number of regressors\n",
    "\n",
    "print(\"Number of observation:\", n, '\\n')\n",
    "print( \"Number of raw regressors:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the outcome variable *wage* and a subset of the raw regressors, we calculate the empirical mean to get familiar with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_subset = data.loc[:, data.columns.isin([\"lwage\",\"sex\",\"shs\",\"hsg\",\"scl\",\"clg\",\"ad\",\"mw\",\"so\",\"we\",\"ne\",\"exp1\"])]\n",
    "table = Z_subset.mean(axis=0)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(data=table, columns={\"Sample mean\":\"0\"} )\n",
    "table.index # saves the values of the average of the variables\n",
    "index1 = list(table.index) #save them in a list \n",
    "index2 = [\"Log Wage\",\"Sex\",\"Some High School\",\"High School Graduate\",\\\n",
    "          \"Some College\",\"College Graduate\", \"Advanced Degree\",\"Midwest\",\\\n",
    "          \"South\",\"West\",\"Northeast\",\"Experience\"]\n",
    "table = table.rename(index=dict(zip(index1,index2))) # names of the constructed data frame and the average values\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The share of female workers in our sample is ~32% ($sex=1$ if female). **This is a mean lower than the one found in the complete data set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also print the table as latex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct a prediction rule for hourly wage $Y$, which depends linearly on job-relevant characteristics $X$:\n",
    "\n",
    "\\begin{equation}\\label{decompose}\n",
    "Y = \\beta'X+ \\epsilon.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we will:\n",
    "\n",
    "* Predict wages  using various characteristics of workers.\n",
    "\n",
    "* Assess the predictive performance using the (adjusted) sample MSE, the (adjusted) sample $R^2$ and the out-of-sample MSE and $R^2$.\n",
    "\n",
    "\n",
    "We employ two different specifications for prediction:\n",
    "\n",
    "\n",
    "1. Basic Model:   $X$ consists of a set of raw regressors (e.g. gender, experience, education indicators,  occupation and industry indicators, regional indicators). That is,  sex + exp1 + shs + hsg+ scl + clg + mw + so + we + occ2+ ind2.\n",
    "\n",
    "\n",
    "2. Flexible Model:  $X$ consists of all raw regressors from the basic model (excepto for sex) plus transformations (e.g., ${exp}^2$, ${exp}^3$ and ${exp}^4$) squared.That is, $(exp1+exp2+exp3+exp4+shs+hsg+scl+clg+occ2+ind2+mw+so+we)^2$\n",
    "\n",
    "\n",
    "Using the **Flexible Model**, enables us to approximate the real relationship by a more complex regression model and therefore to reduce the bias. The **Flexible Model** increases the range of potential shapes of the estimated regression function. In general, flexible models often deliver good prediction accuracy but give models which are harder to interpret. *Also, with flexible models we have the trade off between bias and variance, making it more propense to overfitting.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us fit both models to our data by running ordinary **least squares (ols):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for OLS regression\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. basic model with 51 regressors\n",
    "basic = 'lwage ~ sex + exp1 + shs + hsg+ scl + clg + mw + so + we + occ2+ ind2'\n",
    "basic_results = smf.ols(basic , data=data).fit()\n",
    "print(basic_results.summary()) # estimated coefficients\n",
    "print( \"Number of regressors in the basic model:\",len(basic_results.params), '\\n')  # number of regressors in the Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note that the basic model consists of $51$ regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. flexible model\n",
    "flex = 'lwage ~ (exp1+exp2+exp3+exp4+shs+hsg+scl+clg+occ2+ind2+mw+so+we)**2'\n",
    "flex_results_0 = smf.ols(flex , data=data)\n",
    "flex_results = smf.ols(flex , data=data).fit()\n",
    "print(flex_results.summary()) # estimated coefficients\n",
    "print( \"Number of regressors in the basic model:\",len(flex_results.params), '\\n') # number of regressors in the Flexible Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note that the flexible model consists of $246$ regressors (because we have not only the raw variables and their interactions, but also the squared versions of them.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Lasso next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages for lasso \n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get exogenous variables from flexible model\n",
    "X = flex_results_0.exog #bc skitlearn cannot produce them in the regression\n",
    "X.shape # the subsample data of the people who didnt go to college"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set endogenous variable\n",
    "lwage = data[\"lwage\"]\n",
    "lwage.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $\\alpha$ is $\\lambda$, the penalty value for the number of regressors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use $\\alpha$=0.1 we can see a warning message of no converge. However, it is necesary that we increase the penalty value in order to reduce the number of regressors. Now, we estimated the $\\alpha$ as 0.2, 03, 0.4 (we leave the loop as a commentary in order to not interrupt the run of the script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence = np.array( [0, 0.1, 0.2, 0.3 , 0.4, 0.5, 0.6 ] )\n",
    "\n",
    "#for element in sequence:\n",
    "    #reg = linear_model.Lasso(alpha = element)\n",
    "\n",
    "##LASSO regression for flexible model\n",
    "#reg.fit(X, lwage)\n",
    "#lwage_lasso_fitted = reg.fit(X, lwage).predict( X )\n",
    "\n",
    "## coefficients \n",
    "#reg.coef_\n",
    "#print('Lasso Regression: R^2 score', reg.score(X, lwage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we find that 0.4 makes the model converge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.5\n",
    "#however, we find that 0.39579 is the smallest value of alpha that makes the model converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.Lasso(alpha = alpha)\n",
    "\n",
    "# LASSO regression for flexible model\n",
    "reg.fit(X, lwage)\n",
    "lwage_lasso_fitted = reg.fit(X, lwage).predict( X )\n",
    "\n",
    "# coefficients \n",
    "reg.coef_\n",
    "print('Lasso Regression: R^2 score', reg.score(X, lwage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check predicted values\n",
    "lwage_lasso_fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can evaluate the performance of both models based on the (adjusted) $R^2_{sample}$ and the (adjusted) $MSE_{sample}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Model\n",
    "basic = 'lwage ~ sex + exp1 + shs + hsg+ scl + clg + mw + so + we + occ2+ ind2'\n",
    "basic_results = smf.ols(basic , data=data).fit()\n",
    "\n",
    "# Flexible model \n",
    "flex = 'lwage ~ (exp1+exp2+exp3+exp4+shs+hsg+scl+clg+occ2+ind2+mw+so+we)**2'\n",
    "flex_results = smf.ols(flex , data=data).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the predictive performance\n",
    "R2_1 = basic_results.rsquared\n",
    "print(\"R-squared for the basic model: \", R2_1, \"\\n\")\n",
    "R2_adj1 = basic_results.rsquared_adj\n",
    "print(\"adjusted R-squared for the basic model: \", R2_adj1, \"\\n\")\n",
    "\n",
    "\n",
    "R2_2 = flex_results.rsquared\n",
    "print(\"R-squared for the basic model: \", R2_2, \"\\n\")\n",
    "R2_adj2 = flex_results.rsquared_adj\n",
    "print(\"adjusted R-squared for the basic model: \", R2_adj2, \"\\n\")\n",
    "\n",
    "R2_L = reg.score(flex_results_0.exog, lwage)\n",
    "print(\"R-squared for LASSO: \", R2_L, \"\\n\")\n",
    "R2_adjL = 1 - (1-R2_L)*(len(lwage)-1)/(len(lwage)-X.shape[1]-1)\n",
    "print(\"adjusted R-squared for LASSO: \", R2_adjL, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the MSE\n",
    "MSE1 =  np.mean(basic_results.resid**2)\n",
    "print(\"MSE for the basic model: \", MSE1, \"\\n\")\n",
    "p1 = len(basic_results.params) # number of regressors\n",
    "n = len(lwage)\n",
    "MSE_adj1  = (n/(n-p1))*MSE1\n",
    "print(\"adjusted MSE for the basic model: \", MSE_adj1, \"\\n\")\n",
    "\n",
    "MSE2 =  np.mean(flex_results.resid**2)\n",
    "print(\"MSE for the flexible model: \", MSE2, \"\\n\")\n",
    "p2 = len(flex_results.params) # number of regressors\n",
    "n = len(lwage)\n",
    "MSE_adj2  = (n/(n-p2))*MSE2\n",
    "print(\"adjusted MSE for the flexible model: \", MSE_adj2, \"\\n\")\n",
    "\n",
    "\n",
    "MSEL = mean_squared_error(lwage, lwage_lasso_fitted)\n",
    "print(\"MSE for the LASSO model: \", MSEL, \"\\n\")\n",
    "pL = reg.coef_.shape[0] # number of regressors\n",
    "n = len(lwage)\n",
    "MSE_adjL  = (n/(n-pL))*MSEL\n",
    "print(\"adjusted MSE for LASSO model: \", MSE_adjL, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package for latex table \n",
    "#import array_to_latex as a2l\n",
    "\n",
    "table = np.zeros((3, 5))\n",
    "table[0,0:5] = [p1, R2_1, MSE1, R2_adj1, MSE_adj1]\n",
    "table[1,0:5] = [p2, R2_2, MSE2, R2_adj2, MSE_adj2]\n",
    "table[2,0:5] = [pL, R2_L, MSEL, R2_adjL, MSE_adjL]\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(table, columns = [\"p\",\"$R^2_{sample}$\",\"$MSE_{sample}$\",\"$R^2_{adjusted}$\", \"$MSE_{adjusted}$\"], \\\n",
    "                      index = [\"basic reg\",\"flexible reg\", \"lasso flex\"])\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considering all measures above, the flexible model performs better than the basic model and the lasso flex model**.\n",
    "\n",
    "Also, it is worth noticing that the results in the R scrip won't be equal to the ones found here in the lasso flexible model because the lambda/alpha is different.\n",
    "\n",
    "One procedure to circumvent this issue is to use **data splitting** that is described and applied in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "Measure the prediction quality of the two models via data splitting:\n",
    "\n",
    "- Randomly split the data into one training sample and one testing sample. Here we just use a simple method.\n",
    "- Use the training sample for estimating the parameters of the Basic Model and the Flexible Model.\n",
    "- Use the testing sample for evaluation. Predict the $\\mathtt{lwage}$  of every observation in the testing sample based on the estimated parameters in the training sample.\n",
    "- Calculate the Mean Squared Prediction Error $MSE_{test}$ based on the testing sample for both prediction models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages for splitting data\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "# to make the results replicable (generating random numbers)\n",
    "np.random.seed(0)\n",
    "random = np.random.randint(0,n, size=math.floor(n))\n",
    "data[\"random\"] = random\n",
    "random    # the array does not change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = data.sort_values(by=['random'])\n",
    "data_2.head() #sort the random numbers in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing sample \n",
    "train = data_2[ : math.floor(n*4/5)]    # training sample (75% of the data)\n",
    "test =  data_2[ math.floor(n*4/5) : ]   # testing sample\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Model\n",
    "basic = 'lwage ~ sex + exp1 + shs + hsg+ scl + clg + mw + so + we + occ2+ ind2'\n",
    "basic_results = smf.ols(basic , data=data).fit()\n",
    "\n",
    "# Flexible model \n",
    "flex = 'lwage ~ (exp1+exp2+exp3+exp4+shs+hsg+scl+clg+occ2+ind2+mw+so+we)**2'\n",
    "flex_results = smf.ols(flex , data=data).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic model\n",
    "# estimating the parameters in the training sample\n",
    "basic_results = smf.ols(basic , data=train).fit()\n",
    "print(basic_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwage_test = test[\"lwage\"].values\n",
    "#test = test.drop(columns=['wage', 'lwage', 'random'])\n",
    "lwage_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the out-of-sample MSE\n",
    "test = sm.add_constant(test)   #add constant \n",
    "\n",
    "lwage_pred =  basic_results.predict(test) # predict out of sample\n",
    "print(lwage_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_test1 = np.sum((lwage_test-lwage_pred)**2)/len(lwage_test)\n",
    "R2_test1  = 1 - MSE_test1/np.var(lwage_test)\n",
    "\n",
    "print(\"Test MSE for the basic model: \", MSE_test1, \" \")\n",
    "print(\"Test R2 for the basic model: \", R2_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the basic model, the $MSE_{test}$ is *slightly closed* to the $MSE_{sample}$ when not adjusted. **However, it is slightly more distant if we compared it to the $MSE_{adjusted}$**.\n",
    "\n",
    "Note that this wont be equal to the estimation in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexible model\n",
    "# estimating the parameters in the training sample\n",
    "flex_results = smf.ols(flex , data=train).fit()\n",
    "\n",
    "# calculating the out-of-sample MSE\n",
    "lwage_flex_pred =  flex_results.predict(test) # predict out of sample\n",
    "lwage_test = test[\"lwage\"].values\n",
    "\n",
    "MSE_test2 = np.sum((lwage_test-lwage_flex_pred)**2)/len(lwage_test)\n",
    "R2_test2  = 1 - MSE_test2/np.var(lwage_test)\n",
    "\n",
    "print(\"Test MSE for the flexible model: \", MSE_test2, \" \")\n",
    "print(\"Test R2 for the flexible model: \", R2_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the flexible model **using ols**, the discrepancy between the $MSE_{test}$ and the $MSE_{sample}$ is **large** because the $R^2$ is negative, meaning a bad fit for the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth to notice that the $MSE_{test}$ vary across different data splits. Hence, it is a good idea average the out-of-sample MSE over different data splits to get valid results.\n",
    "\n",
    "Nevertheless, we observe that, based on the out-of-sample $MSE$, the basic model using ols regression performs is about as well (or slightly better) than the flexible model. *Let's remember that the basic model has less regressors*.\n",
    "\n",
    "*Also, let's remember that this results won't be necessary the same with the ones found in the R script because the determination of the data splitting won't be the same.*\n",
    "\n",
    "Next, let us use lasso regression in the flexible model instead of ols regression. Lasso (*least absolute shrinkage and selection operator*) is a penalized regression method that can be used to reduce the complexity of a regression model when the number of regressors $p$ is relatively large in relation to $n$. \n",
    "\n",
    "Note that the out-of-sample $MSE$ on the test sample can be computed for any other black-box prediction method as well. Thus, let us finally compare the performance of lasso regression in the flexible model to ols regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flexible model using lasso\n",
    "# get exogenous variables from training data used in flex model\n",
    "flex_results_0 = smf.ols(flex , data=train)\n",
    "X_train = flex_results_0.exog\n",
    "print(X_train.shape)\n",
    "\n",
    "# Get endogenous variable \n",
    "lwage_train = train[\"lwage\"]\n",
    "print(lwage_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flexible model using lasso\n",
    "\n",
    "# get exogenous variables from testing data used in flex model\n",
    "flex_results_1 = smf.ols(flex , data=test)\n",
    "X_test = flex_results_1.exog\n",
    "print(X_test.shape)\n",
    "\n",
    "# Get endogenous variable \n",
    "lwage_test = test[\"lwage\"]\n",
    "print(lwage_test.shape) # have 30% of the observations in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.Lasso(alpha=0.5)\n",
    "lwage_lasso_fitted = reg.fit(X_train, lwage_train).predict( X_test )\n",
    "\n",
    "MSE_lasso = np.sum((lwage_test-lwage_lasso_fitted)**2)/len(lwage_test)\n",
    "R2_lasso  = 1 - MSE_lasso/np.var(lwage_test)\n",
    "\n",
    "print(\"Test MSE for the flexible model: \", MSE_lasso, \" \")\n",
    "print(\"Test R2 for the flexible model: \", R2_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us summarize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package for latex table \n",
    "#import array_to_latex as a2l\n",
    "\n",
    "table2 = np.zeros((3, 2))\n",
    "table2[0,0] = MSE_test1\n",
    "table2[1,0] = MSE_test2\n",
    "table2[2,0] = MSE_lasso\n",
    "table2[0,1] = R2_test1\n",
    "table2[1,1] = R2_test2\n",
    "table2[2,1] = R2_lasso\n",
    "\n",
    "table2 = pd.DataFrame(table2, columns = [\"$MSE_{test}$\", \"$R^2_{test}$\"], \\\n",
    "                      index = [\"basic reg\",\"flexible reg\",\"lasso regression\"])\n",
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that the lasso regression is slightly better at predicting the data than was before using OLS. However, we can see that, overall, **the basic model is better at predicting the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2.to_latex #in latex\n",
    "print(table2.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. In addition Do two cases of Partialling-Out using lasso. Remember that we want to find the beta associated with sex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Matrix W = 'exp1 + shs + hsg+ scl + clg + mw + so + we + occ2+ ind2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for Y\n",
    "flex_y = 'lwage ~  exp1 + shs + hsg+ scl + clg + mw + so + we + occ2+ ind2'\n",
    "\n",
    "# model for D\n",
    "flex_d = 'sex ~ exp1 + shs + hsg+ scl + clg + mw + so + we + occ2+ ind2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# flex_y\n",
    "\n",
    "#define the lasso model\n",
    "lasso_model = linear_model.Lasso( alpha = 0.5 )\n",
    "\n",
    "#extract the covariable to use\n",
    "flex_y_covariables = smf.ols(formula = flex_y, data = data)\n",
    "\n",
    "#fit hte model and predict the value of the outcome\n",
    "Y_lasso_fitted = lasso_model.fit( flex_y_covariables.exog, data[[ 'lwage' ]] ).predict( flex_y_covariables.exog )\n",
    "\n",
    "#save the residuals\n",
    "t_Y = data[[ 'lwage' ]] - Y_lasso_fitted.reshape( Y_lasso_fitted.size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraflex_d\n",
    "\n",
    "#extract the covariable to use\n",
    "flex_d_covariables = smf.ols( flex_d, data=data)\n",
    "\n",
    "#fit hte model and predict the value of the outcome\n",
    "D_lasso_fitted = lasso_model.fit( flex_d_covariables.exog, data[[ 'sex' ]] ).predict( flex_d_covariables.exog )\n",
    "\n",
    "#save the residuals\n",
    "t_D = data[[ 'sex' ]] - D_lasso_fitted.reshape( D_lasso_fitted.size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a data frame with the residuals of both the regressions\n",
    "data_res = pd.DataFrame( np.hstack(( t_Y , t_D )) , columns = [ 't_Y', 't_D' ] )\n",
    "\n",
    "# regression of the residuals Y on D after partialling-out the effect of W\n",
    "partial_lasso_fit = smf.ols( formula = 't_Y ~ t_D' , data = data_res ).fit()\n",
    "partial_lasso_est = partial_lasso_fit.summary2().tables[1]['Coef.']['t_D']\n",
    "\n",
    "print( f\"Coefficient for D via partialling-out using lasso {partial_lasso_est}\" )\n",
    "\n",
    "# standard error\n",
    "HCV_coefs = partial_lasso_fit.cov_HC0\n",
    "partial_lasso_se = np.power( HCV_coefs.diagonal() , 0.5)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024318,
     "end_time": "2021-02-21T17:17:54.320082",
     "exception": false,
     "start_time": "2021-02-21T17:17:54.295764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The use of the lasso for partialling-out the basic regressions shows us that the gender gap is of $12.14\\%$, which is different from the $7\\%$ gap found int the basic regression using OLS. Probably because lasso supresses some of the regressors but not all of them, and because partialling out controls for the covariates in the both regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025752,
     "end_time": "2021-02-21T17:17:54.682415",
     "exception": false,
     "start_time": "2021-02-21T17:17:54.656663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next we try \"extra\" flexible model, where we take interactions of all controls, giving us about 1000 controls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Matrix W = '(exp1+exp2+exp3+exp4+shs+hsg+scl+clg+occ2+ind2+mw+so+we)**2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets remember that sklearn will not solve $(exp1+exp2+exp3+exp4+shs+hsg+scl+clg+occ2+ind2+mw+so+we)^2$, so we use a previous saved set of exogenous variables called \"X\" that does solve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for Y\n",
    "flex_y = 'lwage ~  X'\n",
    "\n",
    "# model for D\n",
    "flex_d = 'sex ~ X'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# flex_y\n",
    "\n",
    "#define the lasso model\n",
    "lasso_model = linear_model.Lasso( alpha = 0.5 )\n",
    "\n",
    "#extract the covariable to use\n",
    "flex_y_covariables = smf.ols(formula = flex_y, data = data)\n",
    "\n",
    "#fit hte model and predict the value of the outcome\n",
    "Y_lasso_fitted = lasso_model.fit( flex_y_covariables.exog, data[[ 'lwage' ]] ).predict( flex_y_covariables.exog )\n",
    "\n",
    "#save the residuals\n",
    "t_Y = data[[ 'lwage' ]] - Y_lasso_fitted.reshape( Y_lasso_fitted.size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraflex_d\n",
    "\n",
    "#extract the covariable to use\n",
    "flex_d_covariables = smf.ols( flex_d, data=data)\n",
    "\n",
    "#fit hte model and predict the value of the outcome\n",
    "D_lasso_fitted = lasso_model.fit( flex_d_covariables.exog, data[[ 'sex' ]] ).predict( flex_d_covariables.exog )\n",
    "\n",
    "#save the residuals\n",
    "t_D = data[[ 'sex' ]] - D_lasso_fitted.reshape( D_lasso_fitted.size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a data frame with the residuals of both the regressions\n",
    "data_res = pd.DataFrame( np.hstack(( t_Y , t_D )) , columns = [ 't_Y', 't_D' ] )\n",
    "\n",
    "# regression of the residuals Y on D after partialling-out the effect of W\n",
    "partial_lasso_fit = smf.ols( formula = 't_Y ~ t_D' , data = data_res ).fit()\n",
    "partial_lasso_est = partial_lasso_fit.summary2().tables[1]['Coef.']['t_D']\n",
    "\n",
    "print( f\"Coefficient for D via partialling-out using lasso {partial_lasso_est}\" )\n",
    "\n",
    "# standard error\n",
    "HCV_coefs = partial_lasso_fit.cov_HC0\n",
    "partial_lasso_se = np.power( HCV_coefs.diagonal() , 0.5)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024318,
     "end_time": "2021-02-21T17:17:54.320082",
     "exception": false,
     "start_time": "2021-02-21T17:17:54.295764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The use of the lasso for partialling-out the basic regressions shows us that the gender gap is of $11.40\\%$, which is different from the $7\\%$ gap found int the basic regression using OLS but **pretty similar to the $12\\%$ found on the basic regression model with partialling-out.** Above the possible reasons already mentioned, in this flexible model we have more regressors but also controlled by the penalty value $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra2: flexible model with sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_flex = 'lwage ~ sex + (exp1+exp2+exp3+exp4+shs+hsg+scl+clg+occ2+ind2+mw+so+we)**2'\n",
    "\n",
    "control_fit = smf.ols( formula = sex_flex, data=data).fit()\n",
    "\n",
    "print(control_fit.summary()) # estimated coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The $R^2$ of this new flexible model used in all the sample data is $0.51$, which is higher than the flexible model (that had $0.50$). This is due to the addition of one more regressor (sex).**"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
