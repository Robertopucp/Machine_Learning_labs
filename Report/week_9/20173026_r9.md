## ASSIGNMENT 9 
---

> ###  **DOUBLE/DEBIASED MACHINE LEARNING FOR TREATMENT**

> #### *By Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newy, and James Robi (2018)*

______

    - Name: Sandra Mireli Martínez Gutiérrez
    - Student ID: 20173026
______

Authors construct estimates fro low-dimensional target parameters in the presence of high-dimensional nuisance parameters. The set-up introduced in the paper is the following equation:

$ Y = D\theta_{0} + g_{0} + \zeta $, where $E[\zeta | D,X] = 0 $
$ D = m_{0} + V, where E[V|X] = 0 $

The outcome is $Y$ and $D$ is the variable of interest. Also, the vector $X$ consist of other confounding covariates, and $\zeta$ and $V$ are the stochastic errors. Now, lets define the first equation as the equation of interest, in which $\theta_{0}$ is the target parameter.  

Having said that, authors' goal is to estimate the high-dimensional nuisance parameters using machine learning methods, which allow for causal estimation and valid inference of the target parameters. They implement this using lasso and regression forest methods. The key here is constructing the “right” moment conditions to solve, such that small deviations in the nuisance functions do not invalidate the moment conditions. This theoretical underpinnings of the derived moment condition is referred in the paper as the Neyman Orthogonal Score. The paper provides two ways to construct the Neyman Orthogonal Score:
* The Finite Nuisance Parameter Approach 
* The Concentrating Out Approach

## *Finite Nuisance Parameter Approach*

Let $W = (Y, D, X)$. The true values of $\theta$ and $\beta$, denoted as $\theta_{0}$ and $\beta_{0}$, fit the data best, in the sense that 

$$(\theta_{0},\beta_{0}) = argmax E[l(W,\theta,\beta)] $$

where $l(W,\theta,\beta)$  is some criterion (squared eviation, log likelihood etc).
Now, *The Neyman Orthogonal Score* $\psi$ is defined by:

$$ \psi(W,\theta,\beta,\mu)=\dfrac{\partial}{\partial\theta\partial\beta}E_{W}\left[\dfrac{\partial}{\partial\theta\partial\beta}l(W,\theta,\beta)\right]
$$

It is important to say that the vector $\mu$ above is defined by the hessian of this criterion function. Let $J$ be:

$$ J=\left(\begin{array}{cc}
J_{\theta,\theta} & J_{\theta,\beta}\\
J_{\beta,\theta} & J_{\beta,\beta}
\end{array}\right)=\dfrac{\partial}{\partial\theta\partial\beta}E_{W}\left[\dfrac{\partial}{\partial\theta\partial\beta}l(W,\theta,\beta)\right] $$

The vector $\mu$ above is defined by $\mu=J_{\theta,\beta}J_{\beta,\beta^{*}}^{-1}$



In the linear regression, the function $l$ is defined as $ l(W;\theta,\beta) = -\dfrac{(Y-D\theta - X'\beta)^2}{2} $ and the necessary gradients needed for $\psi$ are the following:

$$ \partial\ell_{\theta}(W;\theta,\beta)=(Y-D\theta-X'\beta)D $$

$$ \partial\ell_{\beta}(W;\theta,\beta)=(Y-D\theta-X'\beta)X $$

The entries in the Hessian matrix that we need to compute $\mu$ are:

$$ J_{\theta\beta}=-E[DX'] $$
$$ J_{\beta\beta}=-E[XX'] $$

yielding this expression for $\mu$:

$$ \mu = E[DX'](E[XX'])^{-1}  $$

Finally, the Neyman orthogonal score is then given by:

$$ \psi(W;\theta,\eta) = (Y-D\theta-X'\beta)(D-\mu X) $$


## *Concentrating Out Approach*

Authors say that the approach for constructing Neyman orthogonal scores is closely related to the "concentrating-out approach". For all $\theta \in \Theta $, let $\beta_{\theta}$ be the solution of the following optimization problem:

$$ argmax E[l(W; \theta,\beta)] $$

where $\beta_{theta}$ satisfies $\partial_{\beta}E[l(W; \theta,\beta)] = 0 $

Differentiating this with respect to $\theta$ and interchanging the order of differentiation gives us:

$$ \begin{array}{cc}
0= & \partial_{\theta}\partial_{\beta}E[\ell(W;\theta,\beta_{\theta})]=\partial_{\beta}\partial_{\theta}E[\ell(W;\theta,\beta_{\theta})]\\
0= & \partial_{\beta}E[\partial_{\theta}\ell(W;\theta,\beta_{\theta})]+[\partial_{\theta}\beta_{\theta}]'\partial_{\beta}\ell(W;\theta,\beta_{\theta})]\\
0= & \partial_{\beta}Es[\psi(W;\theta,\beta,\partial_{\theta}\beta_{\theta})]
\end{array}
 $$

Finally, So our score function here is $ \psi(W;\theta,\beta,\partial_{\theta}\beta_{\theta}) = \partial_\theta \ell(W;\theta,\beta) + [\partial_\theta \beta_\theta]' \partial_\beta \ell(W;\theta,\beta) $