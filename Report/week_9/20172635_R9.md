## Report 9
## Double/debiased machine learning for treatment and structural parameters
## Author: Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansens, Whitney Newey and James Robins
## Rosemery Fernandez Sanchez 20172635

The document addresses the problem that occurs in inference on a low-dimensional parameter, named $ \ theta_0 $, when there are the presence of high-dimensional parameters that represent a nuisance. The ML or AA methods yield estimators of θ that are strongly biased. In this sense, the author points out that there are two "ingredients" that are simple, but they make a difference. The first is to use orthogonal Neyman scores and the other is to perform a cross-fit that will result in efficient division of the data. The authors will call the set of methods double or faded ML (DML), where it will give point estimators whose bias is less than for the previously mentioned methods. In this sense, the objective of this study is to develop a procedure to be able to estimate and make inferences about θ when it is found in complex environments, and for that the aforementioned instruments are used.

Regarding the points in favor of the approach used, I could point out the complement that the authors make to the theoretical application with empirical evidence through 3 examples through the use of DML. In particular, the author develops the regularization bias in detail and how these are overcome through orthogonalization (Neyman), as well as cross-fit where he shows how the division of the sample has a role to eliminate bias due to overfitting, avoiding the strong entropy conditions.

Regarding the empirical examples, the first reevaluates the Pennsylvania reemployment bonus study; in the second, the DML method is used to estimate the effects of 401 (k) eligibility, the treatment variable, and 401 (k) participation in net financial assets; and finally, the third where Acemoglu's IV estimate is reviewed, this is about the effects of institutions on economic growth. In the second empirical example, the key problem is the lack of random assignment and in terms of the results, the cross-fit decreases the standard error for all models.