# Report 9: Double/debiased machine learning for treatment and structural parameters

### Student: Diego Alonso Gómez (20171738)
---

The authors of this article revisit the semi-parametric problem of inference on a low-dimensional
parameter ![\theta_0](https://latex.codecogs.com/svg.latex?\theta_0) in the presence of high-dimensional nuisance parameters ![\eta_0](https://latex.codecogs.com/svg.latex?\eta_0). To estimate the last one, they consider the use of statistical or machine learning (ML) methods as a normal posibility used commonly, but they also mention that both regularization bias and overfitting in estimating ![\eta_0](https://latex.codecogs.com/svg.latex?\eta_0) cause a heavy bias in estimators of ![\theta_0](https://latex.codecogs.com/svg.latex?\theta_0) that are obtained by naively ML methods.

So, I consider that the authors' research question is the next one: is it posible to remove the impact of regularization bias and overfitting on estimation of the parameter of interest ![\theta_0](https://latex.codecogs.com/svg.latex?\theta_0) by using the next two ingredients: Neyman-orthogonal moments/scores that have reduced sensitivity with respect to ![\eta_0](https://latex.codecogs.com/svg.latex?\eta_0) and cross-fitting as a form for data-splitting? The answer for this question by the authors 

To show this answer properly, let's resume the mathematical development carried out by the authors:

* First, they define the following two expressions, where Y is the outcome variable, D is the policy/treatment variable of interest, vector X consists of other controls, and U and V are disturbances' terms.
<div align="center"> 

![Y = D \theta_0 + g_0(X) + U](https://latex.codecogs.com/svg.latex?Y&space;=&space;D&space;\theta_0&space;&plus;&space;g_0(X)&space;&plus;&space;U) <br/>

![D = m_0(X) + V](https://latex.codecogs.com/svg.latex?D&space;=&space;m_0(X)&space;&plus;&space;V)

</div>

* Then they mention that the first ‘conventional’ estimator ![\hat {\theta}_0](https://latex.codecogs.com/svg.latex?\hat&space;{\theta}_0) can be seen as a solution to estimating the parameters of interest, but the score function ![\varphi](https://latex.codecogs.com/svg.latex?\varphi), that this is a result of having found ![\hat {\theta}_0](https://latex.codecogs.com/svg.latex?\hat&space;{\theta}_0), gives as result is sensitive to biased estimation of g (i.e the Gateaux derivative operator with respect to g does not vanish).
* Consequently, they consider a second construction that employs an orthogonalized formulation obtained "by directly partialling out the effect of X from D to obtain the orthogonalized regressor ![V = D - m_0(X)](https://latex.codecogs.com/svg.latex?V&space;=&space;D&space;-&space;m_0(X)). As they are solving an auxiliary prediction problem to estimate the
conditional mean of D given X, they are doing what thet call ‘double prediction’ or ‘double machine learning’ (DML). Therefore, the next step they do is to formulate the following debiased ML estimator for $\theta_0$ using the main sample of observations:
<div align="center"> 

![\check {\theta}_0 = \left( \frac{1}{n} \sum_{i \in I} \hat{V}_i D^2_i \right)^{-1} \frac{1}{n} D_i \hat{V}_i (Y_i - \hat {g}_0 (X_i) )](https://latex.codecogs.com/svg.latex?\check&space;{\theta}_0&space;=&space;\left(&space;\frac{1}{n}&space;\sum_{i&space;\in&space;I}&space;\hat{V}_i&space;D^2_i&space;\right)^{-1}&space;\frac{1}{n}&space;D_i&space;\hat{V}_i&space;(Y_i&space;-&space;\hat&space;{g}_0&space;(X_i)&space;)) </div>

*Here we have that the orthogonalized or double/debiased ML estimator given in the last equation can be viewed as a solution to estimating the next equation:

<div align="center"> 

![\frac{1}{n} \sum_{i \in I} \psi(W:\check {\theta}_0, \hat {\eta}_0) = 0](https://latex.codecogs.com/svg.latex?\frac{1}{n}&space;\sum_{i&space;\in&space;I}&space;\psi(W:\check&space;{\theta}_0,&space;\hat&space;{\eta}_0)&space;=&space;0) </div>

* where ![\hat {\eta}_0](https://latex.codecogs.com/svg.latex?\hat&space;{\eta}_0) is the estimator of the nuisance parameter ![\eta_0](https://latex.codecogs.com/svg.latex?\eta_0) and ![\psi](https://latex.codecogs.com/svg.latex?\psi) is an orthogonalized or debiased score function that makes the Gateaux derivative operator ,with respect to ![\eta_0](https://latex.codecogs.com/svg.latex?\eta_0), to vanish when evaluated at the true parameter values.

Next, I would say that one of the article's strengths to answer this question are the next ones. The first one would be that it provides many examples of how using this DML technique such as the estimation od the effects of institutions on economic growth, the effects of unemployment insurance bonus on unemployment duration, and the effect of 401(k) eligibility and participation on net financial asset. The second one would be that they develop, in an understandable way, concepts such as debiased ML or cross-fitting ( which is a practical method to allow to overcome the overfitting/high-complexity phenomena that commonly arise in data analysis based on highly adaptive ML method).
    
On the other hand, the weaknesses of the article ,from my perspective, are two: a) the score that is given as result by the use of DML is only efficient while the function is log-likelihood, and  b) it exists some restrictions problems with the infinite dimensional nuisance parameters.  

Finally, the main contribution of this article —from my perspective— is that it shows both teoretically and with examples how to use set of methods called double/debiased ML (DML). So, it aims to make this method be more used in the near future.






