# REPORT : ESTIMATION AND INFERENCE OF HETEROGENEOUS TREATMENT EFFECTS USING RANDOM FORESTS
### Authors: Wager & Athey

#### Student: M. Thais C. Baz√°n Burgos
# ______________________________________________

The corresponding article deals with the forest method, called by the authors as nonparametric causal forest, which is based on the estimation of treatment effects that provides valid asymptotic theory and statistical inference. Thus, they propose nonparametric methods for heterogeneous treatment estimation that facilitate data selection while maintaining the benefits of asymptotic normality and unbiased estimates with valid confidence intervals. Thanks to this, they obtain consistency and asymptotic Gaussian and centered asymptotic normality that allows estimation of heterogeneous treatment effects. Also that asymptotic normality for the use of random forests can be used in pure prediction contexts, and that the performance of the causal forest algorithm has better bias and variance. In fact, when matching trees with a random undersampling mechanism, the mean square error turns out to be better than classical regression methods.

First, the authors comment that the causal tree and forest method is usually better compared to a single highly optimized tree, because generating several different "good" trees and averaging their predictions helps to reduce variance and smooth the proposed decision boundaries. Second, with respect to "honest" trees and forests, they note that sample splitting procedures can be criticized because they "waste" half of the training data at each estimation step. Still, they perform the forest subsampling mechanism to achieve an honesty that does not waste any data. Thus, although the data point cannot be used for split-selection and single-tree leaf estimation, it can be used to specify the effect estimates of forest structure and treatment. While it is true that making double sample trees was to eliminate bias and have centered confidence intervals, they have found that, empirically, these may be better than standard random trees because of their mean square error.

Third, for the bias and honesty of trees and forests, focus on the lemma that translates into a bound on the bias of a single regression tree. That is, if a forest has an average of independently generated trees, the bias of the forest will be the same as the bias of a single tree. For asymptotic normality of random forests, they establish lower bounds for increasing regression trees, convert incremental weak predictors T to 1-incremental sets by subsampling, using ANOVA decomposition, and establish infinitesimal jackknife consistency for random forests.

Finally, a simulation study is performed to verify that the forest method can be used to construct adequate and asymptotically valid concepts with better confidence intervals than those of non-adaptive methods such as k-NN in finite samples. In fact, the results show that causal forests are successful in maintaining the quadratic error. Moreover, for k-NN procedures, if k is less than 100, the performance is lower than that of causal forests and, a k is 100, it is overwhelmed by bias. Specifically for causal forests, their performance improves when "d" is small, which is explained by the dependence of the variance on the product of the variance of the individual trees and the correlation between them.

Although they can be used for the estimation of the rate of heterogeneous treatment effects, it is necessary to control for bias to achieve better coverage, perhaps through stronger splitting rules. This is because you could choose which feature of the training data to split into. Therefore, the authors themselves recognize the need for improvements and extensions to their current results, as well as the extension to functional estimation and the use of a systematic approach to trimming bounds or correcting for bias that would improve the coverage of confidence intervals of nonparametric estimators.