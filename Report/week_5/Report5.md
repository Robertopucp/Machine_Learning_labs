Tittle: Estimation and Inference of Heterogeneous Treatment Effects using Random 

In this article, they develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. That is, their research question focuses on answering how powerful this Random Forests method can be for estimating heterogeneous treatment effects. 

Among the strengths of the article is that it has an exhaustive literature review on random forests and machine learning, which in the end, is the basics to understand prediction methods such as random forests. It is a very well explained literature review and, in addition, the literature is extended to the topic of heterogeneous treatment effects estimation. In addition, the article provides the code package to perform the demonstration. Personally, I find it very interesting when articles do this. 

Among the main contributions of the paper is (i) the asymptotic normality theory that allows statistical inference using random forest predictions. While other work has established asymptotic properties of particular variants and simplifications of the random forest algorithm, the paper provides the first set of conditions under which the predictions made by random forests are both asymptotically unbiased and Gaussian, thus allowing classical statistical inference. To reduce the bias in the tree predictors and allow valid inference, the causal forest follows an honest approach, where one sample is used to construct the partition and the other sample to estimate the parameter, and (ii) the extension to causal forests proposed in this paper is also new. In general, basically what the authors have done is to derive a valid asymptotic theory for the predictors resulting from the honest tree averages.

Among the next steps, as the authors rightly mention, is to be able to extend the theory to the domain of functional estimation, since for now they only provide pointwise confidence intervals for $\tau(x)$. Another step would be to identify the best mechanism for splitting the data. In general, work can be done to identify methods that produce accurate variance estimates, either with very large or very small samples. 
