## Report 4

The reading attempts to answer the question: How to use Lasso regression to select variables? And the importance of this question arises because when researching a variable of interest, there may be problems in trying to choose which regressors would be important in our regression and which ones could be left out without affecting the power of the regression. This is why the paper suggests the use of Lasso regression as an alternative, and throughout the text a descriptive analysis of how this regression works will be made. 

In my opinion, the article is very complete in its analysis. It makes a brief explanation of the problem and then mentions its solution. It gives the reader the option of working with known data and, it seems to me, also provides the STATA codes. It is an article that fulfills the function of teaching.  So, the author contributes to the question through small sections where he introduces us to the problem. For example, in the first section he discusses the problem of covariate selection, when valid covariates are excluded, the estimated coefficient of interest may be artificially strong or artificially weak, so controlling for covariates is proposed. In an experiment, when using experimental manipulations as independent variables, the experimental dependent variable should not be correlated with any omitted variable, which would prevent bias in the estimation of the effect of the independent experimental variable. However, he also mentions the scarce literature on this and the possible problems of not performing the methodology correctly. That is why, in the next subsection, the authors mention the double lasso approach to help researchers correctly select their variables and avoid type I errors. The approach consists of two steps. The second step serves to verify that the variable to be omitted that is moderately predictive of a dependent variable is not highly predictive of an independent variable. In Lasso regression, unlike OLS regression, there is a penalty term. The penalty term causes the lasso regression to reduce the estimated regression coefficients toward zero and potentially set the coefficients of some variables exactly at zero, which helps reduce overfitting. The lasso, by setting some coefficients to zero, also performs variable selection.However, using lasso regression directly can be problematic. Coefficients estimated by lasso that are actually nonzero are often underestimated, and lasso may erroneously exclude variables with nonzero coefficients, especially variables with moderate effects, so it is recommended to perform the "double lasso" procedure. Step 1 is to fit a lasso regression that predicts the dependent variable, and keep track of variables with non-zero estimated coefficients. Step 2 is to fit a lasso regression predicting the focal independent variable, keeping track of variables with non-zero estimated coefficients, and step 3 is to fit a linear regression of the dependent variable on the focal independent variable, including the covariates ($W_{ik}$) selected in either of the first two steps. 
The authors continue with a subsection where they perform a simulation of the mentioned approach and perform a brief analysis of 4 data sets widely used in the literature, where they conclude that the double lasso approach works well even in demanding situations. 
To continue on the question, the authors or the literature could perform more applicative studies of the "double Lasso" approach. I consider that this has been a mostly instructive article, so I would like to see its use in a much more in-depth case. 



```python

```
