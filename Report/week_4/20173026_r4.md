
## ASSIGNMENT 4 
---

> ###  **USING DOUBLE-LASSO REGRESSION FOR PRINCIPLED VARIABLE SELECTION**

> #### *By Oleg Urminsky, Christian Hansen, and Victor Chernozhukov*

______

    - Name: Sandra Mireli Martínez Gutiérrez
    - Student ID: 20173026
______

The authors of this paper aim to present a two-step method using lasso regression given the problem of principled variable selection for covariates. This latter has its issues in the fact that when it is required to control for covariates, this may lead to either a biased parameter selection or to the correlation of omitted variables with the dependent variable. Then, the analysis could lose power prediction or power support. In this context, double-lasso regression may be useful to the extent that it gives a statistical answer to identify and accept covariables with empirical support for its inclusion or the avoidance of spurious ones. In the end, the authors demonstrate that using this developed method based on lasso regression reduces error and increases statistical power.

The strengths of this paper are the illustrations to verify the usefulness of this method. Also, the application of double-lasso in the analysis from prior literature seems to be a good comparison between the common method and the one proposed in this paper. The advantage of using double-lasso is that its goal is to separate the two effects of Lasso: the first step is related to the variable selection in which many $\beta_{i}$ are setting to zero; the second step is related with the coefficient shrinkage in which even non-zero $\beta_{i}$ are smaller, in absolute value than in unpenalized regression. This procedure has its advantages even without selection because it is possible to avoid over-fitting. Moreover, if we are running a Lasso model with many variables $(p>n)$, then we want to have a large penalty to select a small number of variables. However, this penalty might shrink the selected variables too much – now we are underfitting –. Otherwise, the idea of a relaxed lasso is that we can separate the two effects: we can use a high penalty on the first step to select variables, and a smaller penalty on the second step to shrink them by a smaller amount. 

Given the rapidly increasing popularity of Lasso methods in empirical economic research, it is crucial to better understand the merits and limitations of these new tools, and how they compare to other alternatives such as the high-dimensional OLS-based procedures. One of the limitations of the Lasso method is not being able to select all the relevant control variables. The implications of under-selection for post-double Lasso and the debiased Lasso in linear regression models would constantly be observed until it is not solved. Plus, the under-selection of the Lasso has important implications for other inference methods that rely on Lasso as a first-step estimator. Towards this end, an interesting avenue for future research would be to investigate the impact of under-selection on the performance of the Lasso-based approaches for non-linear models. In moderately high-dimensional settings where $p$ is smaller than but comparable to $n$, it would also be interesting to compare the treatment effects estimators and the robust finite sample methods.

Finally, this paper motivates further examinations of the practical usefulness of double-Lasso inference procedures and other modern high-dimensional methods. It would be interesting to explore the implications of their theoretical results on the under-selection of the Lasso in problems with weak instruments.
