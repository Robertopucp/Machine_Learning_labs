## ASSIGNMENT 8 
---

> ###  **RECURSIVE PARTITIONING FOR HETEROGENEOUS CAUSAL EFFECTS**

> #### *By Susan Athey & Guido Imbens (2015)*

______

    - Name: Sandra Mireli Martínez Gutiérrez
    - Student ID: 20173026
______


In this paper, authors focus on implementing the causal tree algorithm. This algorithm structure is guided heavily by the classification and regression tree algorithm. Hence, in step one their project starts by building a working implementation of the classical (regression) decision tree algorithm. Having verified the prediction capabilities of its implementation, they devote their attention to the causal tree algorithm. 

For this algorithm, sample is given by randomization. Also, there is a recursive partitioning for heterogeneous causal effects. Then, as for transforming outcomes into trees, the advantages are:

* It's a way to try to find mapping of inputs.  
* Generalize treatment with probability $p$.
* After the transformation of the outcomes, you can have the expected treatment effect.
* With any supervised machine learning approach, you can try to find a mapping of your covariates to predict heterogeneous treatment effects.
* Splitting covariate space, it is common to apply cost-complexity pruning.
* This method tries to find optimal split of the covariate space. So you can have minimum criteria ($MSE$ + stability via penalization when going too deep and overfitting)
* For each leaf of the tree, you will know the $Y_i^*$ and you will eval prediction of $\tau_i^*$.


However, the disadvantages are the following:

* It is possible to have a leaf that is a poor estimator of $\tau_i$.
* Instead of estimating our individual treatment effect, you can sample average treatment effect within each leaf 
* It can get to be outperformed by the causal tree algorithm 
* Instead of prediction, you can have the treatment effect estimate.

As for the Honest and Adaptive Stimations, each of them requires to adjust the criteria. Here, the Adaptive uses model selection via and fit via same training data creating a bias. Hence, spurious correlation between covariates and outcomes affect the selected model. However, in the end sample mean is more extreme than entire population due to training data.

On the other hand, there is and argue in which the conclution is to better use honest algorithm proceeding to steps. One part of the data is used for the tree and another part which is arguable independent is to estimate treatment effects within leaves of the tree. This eliminates bias but increases the variance of the estimators. Having said that, even if you use Lasso to select the model and then do inference on same training data, you will run into same error. The recommendation here is to focus less on bias but more in variance focus because honest makes bias irrelevant.