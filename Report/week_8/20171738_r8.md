# Report 8: Recursive Partitioning for Heterogeneous Causal Effects

## Diego Alonso Gómez (20171738)

## Final Document. I was delivered somewhat late due to I arriaved late to my house. The reason was ocassionated by delays in the vaccination process at Jockey Plaza to get my second dose of Pfizer.

----

The authors of this article report new methods to estimate the heterogeneity of causal effects in both experimental and observational studies, using a regression tree approach that divides the data into subpopulations, in such a way. way subpopulations is selected to estimate more precisely average treatment effects and to test hypotheses about the differences between the effects in different subpopulations. Besides that, the estimation is done without restricting how many covariates are included in the model. For the authors, this is done through the use and estimation of regression trees, for which is necessary to use what the authors propose as the honest approach. This honest apporach guarantee that there are different samples used for selecting and estimating the model structure, meanwhile the latter uses training data.

So, in this paper are introduced new methods for constructing trees for causal effects that allow us to do valid inference both for causal effects in randomized experiments and for observational studies to make them satisfy unconfoundedness; which are the following options:

a) Causal Tres: this option is used to estimate the conditional mean treatment effects , so the adaptive version of causal trees the authors use for splitting the objective this form of mean-squared error:  $\hat {MSE} (S^{tr,cv},S^{tr},\Pi)$; and for cross-validation they use the same objective function, but evaluated at the samples $S^{tr,cv}$ and $S^{tr,tr}$. All this for finding the "Causal Tree" (CT) estimatorsm which could take this form:
$$
    μ(w, x; S,\Pi) ≡ \frac{1}{\# ({i ∈ S_w : X_i ∈ ℓ(x;\Pi)})} \sum_{i\in S_w: X_i \in ℓ(x,\Pi)} Y_i^{obs}\\[0.4cm]
    \hat {\tau}(x;S;\Pi) = \hat {\mu}(0,x,S,\Pi)
$$

b) Transformed Outcome Trees:  this alternative method is based on the insight that, by using a transformed version of the outcome $Y_i^* = (Y_i − W_i)/(p·(1 − p))$, it's possible to use "off-the-shelf regression tree methods" to focus splitting and cross-validation on treatment effects rather than outcomes.

c) Fit-based Trees (F): for this alternative the authors estimate a model for treatment effects within each leaf of the causal tree, so that they have a linear model with an intercept and an indicator for the treatment as the regressors, rather only an intercept as in standard CART. Therefore, the authors modify the mean-squared error function so that it takes the following form:

$$
MSE_{\mu,W}(S^{te}, S^{est},\Pi) = \sum_{i \in S^{te}}((Y_i^{obs} − \hat {μ}_w(W_i,X_i; S^{est},\Pi))^2−Y_i^2)
$$

d) Squared T-statistic Trees:  this estimator look for splits with the largest value for the square of the t-statistic, all this for testing the null hypothesis that the average treatment effect is the same in the two potential leaves. It basically takes this form: 
$$
    T^2 ≡ N * \frac{(\bar Y_L - \bar Y_R)^2}{S^2/N_L+S^2/N_R}
$$

Finally, one of the main strengths of the article is that it provides inferences comparing the honest estimate with the traditional estimate, as well as inferences for the effects of treatment, showing and explaning a different set of alternatives that allow to calculate different treatment effects and confidence intervals for each subspace. Therefore, as these methods can completely be applied to randomized experiments and observational studies, one next step could consist of using these methods for other academic areas such as medicine, sociology, etc.


